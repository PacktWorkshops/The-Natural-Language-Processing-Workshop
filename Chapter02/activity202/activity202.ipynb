{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting General Features from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "nltk.download('tagsets')\n",
    "from nltk.data import load\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagsets():\n",
    "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "    return list(tagdict.keys())\n",
    " \n",
    "tag_list = get_tagsets()\n",
    " \n",
    "print(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will count occurrence of pos tags in each sentence.\n",
    "def get_pos_occurrence_freq(data, tag_list):\n",
    "    # Get list of sentences in text_list\n",
    "    text_list = data.text\n",
    "    \n",
    "    # create empty dataframe\n",
    "    feature_df = pd.DataFrame(columns=tag_list)\n",
    "    for text_line in text_list:\n",
    "        \n",
    "        # get pos tags of each word.\n",
    "        pos_tags = [j for i, j in pos_tag(word_tokenize(text_line))]\n",
    "        \n",
    "        # create a dict of pos tags and their frequency in given sentence.\n",
    "        row = dict(Counter(pos_tags))\n",
    "        feature_df = feature_df.append(row, ignore_index=True)\n",
    "    feature_df.fillna(0, inplace=True)\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data_ch2/data.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = get_pos_occurrence_freq(data, tag_list)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_punctuation_count(feature_df, data):\n",
    "    # The below code line will find the intersection of set\n",
    "    # of punctuations in text and punctuation set\n",
    "    # imported from string module of python and find the length of\n",
    "    # intersection set in each row and add it to column `num_of_unique_punctuations`\n",
    "    # of data frame.\n",
    " \n",
    "    feature_df['num_of_unique_punctuations'] = data['text']. \\\n",
    "        apply(lambda x: len(set(x).intersection(set(punctuation))))\n",
    "    return feature_df\n",
    " \n",
    "feature_df = add_punctuation_count(feature_df, data)\n",
    " \n",
    "feature_df['num_of_unique_punctuations'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capitalized_word_count(feature_df, data):\n",
    "    # The below code line will tokenize text in every row and\n",
    "    # create a set of only capital words, then find the length of\n",
    "    # this set and add it to the column `number_of_capital_words`\n",
    "    # of dataframe.\n",
    " \n",
    "    feature_df['number_of_capital_words'] = data['text'].\\\n",
    "        apply(lambda x: len([word for word in word_tokenize(str(x)) if word[0].isupper()]))\n",
    "    return feature_df\n",
    " \n",
    "feature_df = get_capitalized_word_count(feature_df, data)\n",
    " \n",
    "feature_df['number_of_capital_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_small_word_count(feature_df, data):\n",
    "    # The below code line will tokenize text in every row and\n",
    "    # create a set of only small words, then find the length of\n",
    "    # this set and add it to the column `number_of_small_words`\n",
    "    # of dataframe.\n",
    " \n",
    "    feature_df['number_of_small_words'] = data['text'].\\\n",
    "        apply(lambda x: len([word for word in word_tokenize(str(x)) if word[0].islower()]))\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = get_small_word_count(feature_df, data)\n",
    "feature_df['number_of_small_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_alphabets(feature_df, data):\n",
    "    # The below code line will break the text line in a list of\n",
    "    # characters in each row and add the count of that list into\n",
    "    # the columns `number_of_alphabets`\n",
    " \n",
    "    feature_df['number_of_alphabets'] = data['text']. \\\n",
    "        apply(lambda x: len([ch for ch in str(x) if ch.isalpha()]))\n",
    "    return feature_df\n",
    "feature_df = get_number_of_alphabets(feature_df, data)\n",
    "feature_df['number_of_alphabets'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_digit_count(feature_df, data):\n",
    "    # The below code line will break the text line in a list of\n",
    "    # digits in each row and add the count of that list into\n",
    "    # the columns `number_of_digits`\n",
    " \n",
    "    feature_df['number_of_digits'] = data['text']. \\\n",
    "        apply(lambda x: len([ch for ch in str(x) if ch.isdigit()]))\n",
    "    return feature_df\n",
    "feature_df = get_number_of_digit_count(feature_df, data)\n",
    "feature_df['number_of_digits'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_words(feature_df, data):\n",
    "    # The below code line will break the text line in a list of\n",
    "    # words in each row and add the count of that list into\n",
    "    # the columns `number_of_digits`\n",
    " \n",
    "    feature_df['number_of_words'] = data['text'].apply(lambda x\n",
    "                                                       : len(word_tokenize(str(x))))\n",
    " \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = get_number_of_words(feature_df, data)\n",
    "feature_df['number_of_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_whitespaces(feature_df, data):\n",
    "    # The below code line will generate list of white spaces\n",
    "    # in each row and add the length of that list into\n",
    "    # the columns `number_of_white_spaces`\n",
    " \n",
    "    feature_df['number_of_white_spaces'] = data['text']. \\\n",
    "        apply(lambda x: len([ch for ch in str(x) if ch.isspace()]))\n",
    " \n",
    "    return feature_df\n",
    " \n",
    "feature_df = get_number_of_whitespaces(feature_df, data)\n",
    "feature_df['number_of_white_spaces'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below cell is for testing different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a test data frame\n",
    "test_df =  pd.DataFrame([{\"text\":\"this is  a cat\"}, {\"text\":\"why are you so happy\"}])\n",
    "tag_list = ['JJR', 'CC', 'VBN', 'CD', 'NNS']\n",
    "\n",
    "# 1) TEST get_pos_occurrence_freq()\n",
    "test_feature_df = get_pos_occurrence_freq(test_df, tag_list)\n",
    "result_df= pd.DataFrame([{\"JJR\":0, \"CC\":0, \"VBN\":0, \"CD\":0, \"NNS\":0,\n",
    "                          \"DT\":2.0,\"NN\":1.0,\"VBZ\":1.0, \"JJ\":0.0,\"PRP\":0.0,\"RB\":0.0,\"VBP\":0.0,\"WRB\":0.0},\n",
    "                        {\"JJR\":0, \"CC\":0, \"VBN\":0, \"CD\":0, \"NNS\":0,\n",
    "                          \"DT\":0.0,\"NN\":0.0,\"VBZ\":0.0, \"JJ\":1.0,\"PRP\":1.0,\"RB\":1.0,\"VBP\":1.0,\"WRB\":1.0}])\n",
    "\n",
    "# Assert equality of the results and expected data frames\n",
    "\n",
    "pd.testing.assert_frame_equal(test_feature_df, result_df,check_names=False,check_like=True)\n",
    "\n",
    "\n",
    "# 2) TEST add_punctuation_count()\n",
    "add_punctuation_count(test_feature_df, test_df)\n",
    "\n",
    "result_df['num_of_unique_punctuations'] = [0,0]\n",
    "pd.testing.assert_frame_equal(add_punctuation_count(test_feature_df, test_df), result_df,check_names=False,check_like=True)\n",
    "\n",
    "# 3) TEST get_capitalized_word_count()\n",
    "get_capitalized_word_count(test_feature_df, test_df)\n",
    "\n",
    "result_df['number_of_capital_words'] = [0,0]\n",
    "\n",
    "pd.testing.assert_frame_equal(get_capitalized_word_count(test_feature_df, test_df), result_df,check_names=False,check_like=True)\n",
    "\n",
    "# 4) TEST get_small_word_count()\n",
    "\n",
    "result_df['number_of_small_words'] = [4,5]\n",
    "get_small_word_count(test_feature_df, test_df)\n",
    "\n",
    "pd.testing.assert_frame_equal(get_small_word_count(test_feature_df, test_df), result_df,check_names=False,check_like=True)\n",
    "\n",
    "# 5) TEST get_number_of_alphabets()\n",
    "\n",
    "get_number_of_alphabets(test_feature_df, test_df)\n",
    "\n",
    "result_df['number_of_alphabets'] = [10,16]\n",
    "\n",
    "pd.testing.assert_frame_equal(get_number_of_alphabets(test_feature_df, test_df), result_df,check_names=False,check_like=True)\n",
    "\n",
    "\n",
    "# 6) get_number_of_digit_count()\n",
    "\n",
    "get_number_of_digit_count(test_feature_df, test_df)\n",
    "\n",
    "result_df['number_of_digits'] = [0,0]\n",
    "\n",
    "pd.testing.assert_frame_equal(get_number_of_digit_count(test_feature_df, test_df), result_df,check_names=False,check_like=True)\n",
    "\n",
    "# 7) TEST get_number_of_words()\n",
    "\n",
    "get_number_of_words(test_feature_df, test_df)\n",
    "\n",
    "result_df['number_of_words'] = [4,5]\n",
    "\n",
    "pd.testing.assert_frame_equal(get_number_of_words(test_feature_df, test_df), result_df,check_names=False,check_like=True)\n",
    "\n",
    "# 8) TEST get_number_of_whitespaces()\n",
    "\n",
    "get_number_of_whitespaces(test_feature_df, test_df)\n",
    "\n",
    "result_df['number_of_white_spaces'] = [4,4]\n",
    "\n",
    "pd.testing.assert_frame_equal(get_number_of_whitespaces(test_feature_df, test_df), result_df,check_names=False,check_like=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
